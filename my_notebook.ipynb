{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e03658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7956/831263909.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['keyword'].fillna('', inplace=True)\n",
      "/tmp/ipykernel_7956/831263909.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_data['location'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7429943955164131\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "train_data = pd.read_csv('./input/train.csv')\n",
    "\n",
    "# Fill missing values\n",
    "train_data['keyword'].fillna('', inplace=True)\n",
    "train_data['location'].fillna('', inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X = train_data[['text', 'keyword', 'location']]\n",
    "y = train_data['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessor\n",
    "categorical_features = ['keyword', 'location']\n",
    "text_features = 'text'\n",
    "\n",
    "# OneHotEncoder setup\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# TF-IDF setup\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "\n",
    "# ColumnTransformer to apply OneHotEncoder to categorical features and TF-IDF to text\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', one_hot_encoder, categorical_features),\n",
    "        ('text', tfidf_vectorizer, text_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Define the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Explanation:\n",
    "# 1. We use OneHotEncoder without the 'sparse' argument, as it is deprecated in newer versions of scikit-learn.\n",
    "# 2. We use TfidfVectorizer to transform the text data into numerical features.\n",
    "# 3. We use a Logistic Regression model with n_jobs=-1 to utilize all available CPU cores for training.\n",
    "# 4. We evaluate the model using the F1 score, which is the challenge's evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47974b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.cache/pypoetry/virtualenvs/kaggle-agent-poet-fT6iIz0K-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)  \u001b[38;5;66;03m# Retain 95% of variance\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Combine feature extraction and dimensionality reduction\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m full_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     45\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_extraction_pipeline),\n\u001b[1;32m     46\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[1;32m     47\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m, pca)\n\u001b[1;32m     48\u001b[0m ])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Transform the training data\u001b[39;00m\n\u001b[1;32m     51\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m full_pipeline\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "# Custom transformer for extracting features from text using BERT\n",
    "class BERTTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=128):\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = transformers.BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        inputs = self.tokenizer(X.tolist(), return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "# Function to extract keyword and location correlation features\n",
    "def extract_correlation_features(X):\n",
    "    keyword_corr = X['keyword'].apply(lambda x: len(x))  # Example: length of keyword\n",
    "    location_corr = X['location'].apply(lambda x: len(x))  # Example: length of location\n",
    "    return np.vstack((keyword_corr, location_corr)).T\n",
    "\n",
    "# Define the feature extraction pipeline\n",
    "feature_extraction_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "        ('bert', BERTTransformer()),\n",
    "        ('correlation', FunctionTransformer(extract_correlation_features, validate=False))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "\n",
    "# Combine feature extraction and dimensionality reduction\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('features', feature_extraction_pipeline),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', pca)\n",
    "])\n",
    "\n",
    "# Transform the training data\n",
    "X_train_transformed = full_pipeline.fit_transform(X_train['text'])\n",
    "\n",
    "# Explanation:\n",
    "# 1. We use TF-IDF and BERT for feature extraction from text, capturing both frequency and semantic meaning.\n",
    "# 2. We create a custom transformer for BERT to extract contextual features from text.\n",
    "# 3. We analyze 'keyword' and 'location' for correlation with the target by creating simple features based on their length.\n",
    "# 4. We apply PCA to reduce the dimensionality of the feature space, retaining 95% of the variance.\n",
    "# 5. We normalize the features using StandardScaler to ensure consistent input scales for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e496c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, n_jobs=-1, class_weight='balanced'),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(kernel='linear', class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_jobs=-1, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate models using cross-validation\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_transformed, y_train, cv=5, scoring='f1')\n",
    "    print(f\"{model_name} F1 Score: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Explanation:\n",
    "# 1. We use Logistic Regression, Naive Bayes, and SVM as traditional machine learning models for text classification.\n",
    "# 2. We include Random Forest and Gradient Boosting as ensemble methods to establish baseline performance.\n",
    "# 3. We use class_weight='balanced' to handle class imbalance in the target variable.\n",
    "# 4. We evaluate models using cross-validation with F1 score as the metric, which is suitable for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Define a simple BERT-based classifier\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', num_classes=2):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "\n",
    "# Prepare data for PyTorch\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train['text'], y_train, test_size=0.2, stratify=y_train)\n",
    "\n",
    "# Tokenize and encode sequences in the training set\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(y_train.values))\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor(y_val.values))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = BERTClassifier()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 5\n",
    "best_val_f1 = 0\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    print(f\"Epoch {epoch+1}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Explanation:\n",
    "# 1. We use a BERT-based classifier for text classification, leveraging pre-trained language models.\n",
    "# 2. We tokenize and encode the text data using BERT tokenizer, preparing it for input to the model.\n",
    "# 3. We implement a training loop with early stopping based on validation F1 score to prevent overfitting.\n",
    "# 4. We use Adam optimizer with a learning rate of 2e-5, suitable for fine-tuning BERT models.\n",
    "# 5. We log training loss and validation F1 score for each epoch to monitor model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_val, y_val):\n",
    "    # Get predictions and prediction probabilities\n",
    "    y_pred = model.predict(X_val)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Example usage with a trained model and validation data\n",
    "# evaluate_model_performance(trained_model, X_val_transformed, y_val)\n",
    "\n",
    "# Explanation:\n",
    "# 1. We define a function `evaluate_model_performance` to calculate and print various evaluation metrics.\n",
    "# 2. We use scikit-learn's metrics functions to compute accuracy, precision, recall, F1-score, and AUC.\n",
    "# 3. We handle both probability outputs and decision scores for AUC calculation, depending on the model.\n",
    "# 4. We print the confusion matrix to help identify common misclassification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a scorer for F1 score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Define hyperparameter grids for different models\n",
    "param_grid_lr = {\n",
    "    'clf__C': np.logspace(-4, 4, 20),\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "param_grid_svc = {\n",
    "    'clf__C': np.logspace(-4, 4, 20),\n",
    "    'clf__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [100, 200, 300],\n",
    "    'clf__max_depth': [None, 10, 20, 30],\n",
    "    'clf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    'clf__alpha': np.logspace(-4, 1, 20)\n",
    "}\n",
    "\n",
    "# Define pipelines for each model\n",
    "pipeline_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline_svc = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', SVC(class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', n_jobs=-1))\n",
    "])\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Perform Randomized Search CV for each model\n",
    "random_search_lr = RandomizedSearchCV(pipeline_lr, param_grid_lr, n_iter=50, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=1)\n",
    "random_search_svc = RandomizedSearchCV(pipeline_svc, param_grid_svc, n_iter=50, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=1)\n",
    "random_search_rf = RandomizedSearchCV(pipeline_rf, param_grid_rf, n_iter=50, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=1)\n",
    "random_search_nb = RandomizedSearchCV(pipeline_nb, param_grid_nb, n_iter=50, scoring=f1_scorer, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit models\n",
    "random_search_lr.fit(X_train['text'], y_train)\n",
    "random_search_svc.fit(X_train['text'], y_train)\n",
    "random_search_rf.fit(X_train['text'], y_train)\n",
    "random_search_nb.fit(X_train['text'], y_train)\n",
    "\n",
    "# Print best parameters and scores\n",
    "print(\"Logistic Regression best params:\", random_search_lr.best_params_)\n",
    "print(\"Logistic Regression best F1 score:\", random_search_lr.best_score_)\n",
    "\n",
    "print(\"SVC best params:\", random_search_svc.best_params_)\n",
    "print(\"SVC best F1 score:\", random_search_svc.best_score_)\n",
    "\n",
    "print(\"Random Forest best params:\", random_search_rf.best_params_)\n",
    "print(\"Random Forest best F1 score:\", random_search_rf.best_score_)\n",
    "\n",
    "print(\"Naive Bayes best params:\", random_search_nb.best_params_)\n",
    "print(\"Naive Bayes best F1 score:\", random_search_nb.best_score_)\n",
    "\n",
    "# Explanation:\n",
    "# 1. We use RandomizedSearchCV to perform hyperparameter tuning for different models.\n",
    "# 2. We define parameter grids for Logistic Regression, SVC, Random Forest, and Naive Bayes.\n",
    "# 3. We use a pipeline to integrate TF-IDF vectorization and model training.\n",
    "# 4. We use F1 score as the evaluation metric, suitable for imbalanced datasets.\n",
    "# 5. We print the best parameters and corresponding F1 scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Load test data\n",
    "# Assuming test data is loaded in a DataFrame called `test_data`\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "test_data['keyword'] = imputer.fit_transform(test_data[['keyword']])\n",
    "test_data['location'] = imputer.fit_transform(test_data[['location']])\n",
    "\n",
    "# Encode categorical data\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encoded_keywords = encoder.fit_transform(test_data[['keyword']])\n",
    "encoded_locations = encoder.fit_transform(test_data[['location']])\n",
    "\n",
    "# Feature extraction from text\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_test_text = vectorizer.fit_transform(test_data['text'])\n",
    "\n",
    "# Combine all features\n",
    "X_test_combined = np.hstack((X_test_text.toarray(), encoded_keywords, encoded_locations))\n",
    "\n",
    "# Load the best model\n",
    "# Assuming the best model is saved as `best_model.pkl`\n",
    "import joblib\n",
    "best_model = joblib.load('best_model.pkl')\n",
    "\n",
    "# Predict on test data\n",
    "predictions = best_model.predict(X_test_combined)\n",
    "\n",
    "# Post-process predictions if necessary\n",
    "# For example, adjust threshold if needed\n",
    "# predictions = (best_model.predict_proba(X_test_combined)[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'target': predictions})\n",
    "submission.to_csv('./output/submission.csv', index=False)\n",
    "\n",
    "# Explanation:\n",
    "# 1. We handle missing values in 'keyword' and 'location' using SimpleImputer.\n",
    "# 2. We encode 'keyword' and 'location' using OneHotEncoder.\n",
    "# 3. We extract features from 'text' using TF-IDF.\n",
    "# 4. We load the best model and predict on the test data.\n",
    "# 5. We prepare the submission file according to the competition's requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-agent-poet-fT6iIz0K-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
